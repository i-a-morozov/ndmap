{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "316164ad-9e27-492f-aac4-c54c6c9c7649",
   "metadata": {},
   "source": [
    "# Example-01: Derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "958fa649-25fb-42cd-84cf-e22834e90ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given an input function, its higher order (partial) derivatives with respect to one or sevaral tensor arguments can be computed using forward or reverse mode automatic differentiation\n",
    "# Derivative orders can be different for each tensor argument\n",
    "# Input function is expected to return a tensor or a (nested) list of tensors\n",
    "\n",
    "# Derivatives are computed by nesting functorch jacobian functions\n",
    "# For higher order derivatives, this results in growing redundant computations, forward mode is more efficient in this case\n",
    "\n",
    "# If the input function returns a tensor, the output is referred as derivative table representation\n",
    "# This representation can be evaluated near given evaluation point if the input function returns a scalar or a vector\n",
    "# Table representation is a (nested) list of tensors, it can be used as a (redundant) function representation near given evaluation point\n",
    "# Table structure for f(x), f(x, y) and f(x, y, z) is shown bellow (similar structure holds for a function with more aruments)\n",
    "\n",
    "# f(x)\n",
    "# t(f, x)\n",
    "# [f, Dx f, Dxx f, ...]\n",
    "\n",
    "# f(x, y)\n",
    "# t(f, x, y)\n",
    "# [\n",
    "#     [    f,     Dy f,     Dyy f, ...],\n",
    "#     [ Dx f,  Dx Dy f,  Dx Dyy f, ...],\n",
    "#     [Dxx f, Dxx Dy f, Dxx Dyy f, ...],\n",
    "#     ...\n",
    "# ]\n",
    "\n",
    "# f(x, y, z)\n",
    "# t(f, x, y, z)\n",
    "# [\n",
    "#     [\n",
    "#         [         f,          Dz f,          Dzz f, ...],\n",
    "#         [      Dy f,       Dy Dz f,       Dy Dzz f, ...],\n",
    "#         [     Dyy f,      Dyy Dz f,      Dyy Dzz f, ...],\n",
    "#         ...\n",
    "#     ],\n",
    "#     [\n",
    "#         [      Dx f,       Dx Dz f,       Dx Dzz f, ...],\n",
    "#         [   Dx Dy f,    Dx Dy Dz f,    Dx Dy Dzz f, ...],\n",
    "#         [  Dx Dyy f,   Dx Dyy Dz f,   Dx Dyy Dzz f, ...],\n",
    "#         ...\n",
    "#     ],\n",
    "#     [\n",
    "#         [    Dxx f,     Dxx Dz f,     Dxx Dzz f, ...],\n",
    "#         [ Dxx Dy f,  Dxx Dy Dz f,  Dxx Dy Dzz f, ...],\n",
    "#         [Dxx Dyy f, Dxx Dyy Dz f, Dxx Dyy Dzz f, ...],\n",
    "#         ...\n",
    "#     ],\n",
    "#     ...\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8923ac82-cc4e-4bc0-a196-b30ce2474913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Import\n",
    "\n",
    "import torch\n",
    "import functorch\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import ndtorch.ndtorch as nd\n",
    "\n",
    "torch.set_printoptions(precision=12, sci_mode=True)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49f1ae45-fcc0-4663-9f66-f3b01d9b1c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set data type and device\n",
    "\n",
    "dtype = torch.float64\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c85724c-981d-4a2e-b61b-51e24259a442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic derivative interface\n",
    "\n",
    "# nd.derivative(\n",
    "#     order:int,                            # derivative order\n",
    "#     function:Callable,                    # input function\n",
    "#     *args,                                # function(*args) = function(x:Tensor, ...)\n",
    "#     intermediate:bool = True,             # flag to return intermediate derivatives\n",
    "#     jacobian:Callable = functorch.jacfwd  # functorch.jacfwd or functorch.jacfrev\n",
    "# )\n",
    "\n",
    "# nd.derivative(\n",
    "#     order:tuple[int, ...],                # derivative orders\n",
    "#     function:Callable,                    # input function\n",
    "#     *args,                                # function(*args) = function(x:Tensor, y:Tensor, z:Tensor, ...)\n",
    "#     intermediate:bool = True,             # flag to return intermediate derivatives\n",
    "#     jacobian:Callable = functorch.jacfwd  # functorch.jacfwd or functorch.jacfrev\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd7b6d08-4672-4de0-82ad-c68ad2e129df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120.0\n",
      "1.0, 1.0, 2.0, 6.0, 24.0, 120.0\n",
      "6.0\n",
      "6.0\n"
     ]
    }
   ],
   "source": [
    "# Derivative\n",
    "\n",
    "# Input:  scalar\n",
    "# Output: scalar\n",
    "\n",
    "# Set test function\n",
    "# Note, the first function argument is a scalar tensor\n",
    "# Input function can have other additional arguments\n",
    "# Other arguments are not used in computation of derivatives\n",
    "\n",
    "def fn(x, a, b, c, d, e, f):\n",
    "    return a + b*x + c*x**2 + d*x**3 + e*x**4 + f*x**5\n",
    "\n",
    "# Set derivative order\n",
    "\n",
    "n = 5\n",
    "\n",
    "# Set evaluation point\n",
    "\n",
    "x = torch.tensor(0.0, dtype=dtype, device=device)\n",
    "\n",
    "# Set fixed parameters\n",
    "\n",
    "a, b, c, d, e, f = torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0, 1.0], dtype=dtype, device=device)\n",
    "\n",
    "# Compute n'th derivative\n",
    "\n",
    "value = nd.derivative(n, fn, x, a, b, c, d, e, f, intermediate=False, jacobian=functorch.jacfwd)\n",
    "print(value.cpu().numpy().tolist())\n",
    "\n",
    "# Compute all derivatives upto given order\n",
    "# Note, function value itself is referred as zeros order derivative\n",
    "# Since function returns a tensor, output is a list of tensors\n",
    "\n",
    "values = nd.derivative(n, fn, x, a, b, c, d, e, f, intermediate=True, jacobian=functorch.jacfwd)\n",
    "print(*[value.cpu().numpy().tolist() for value in values], sep=', ')\n",
    "\n",
    "# Note, intermediate flag (default=True) can be used to return all derivatives\n",
    "# For jacobian parameter, functorch.jacfwd or functorch.jacrev functions can be passed\n",
    "\n",
    "# Evaluate derivative table representation for a given deviation from the evaluation point\n",
    "\n",
    "dx = torch.tensor(1.0, dtype=dtype, device=device)\n",
    "print(nd.evaluate(nd.derivative(n, fn, x, a, b, c, d, e, f) , [dx]).cpu().numpy().tolist())\n",
    "print(fn(x + dx, a, b, c, d, e, f).cpu().numpy().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae6bddd2-2ed5-4656-aac8-3aa520f69fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.0, 0.0], [0.0, 2.0]]\n",
      "3.0, [-2.0, 2.0], [[2.0, 0.0], [0.0, 2.0]]\n",
      "3.0, [-2.0, 2.0], [[2.0, 0.0], [0.0, 2.0]]\n",
      "1.0\n",
      "1.0\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "[[-2.0, 2.0], [-2.0, 2.0], [-2.0, 2.0], [-2.0, 2.0], [-2.0, 2.0]]\n"
     ]
    }
   ],
   "source": [
    "# Derivative\n",
    "\n",
    "# Input:  vector\n",
    "# Output: scalar\n",
    "\n",
    "# Set test function\n",
    "# Note, the first function argument is a vector tensor\n",
    "# Input function can have other additional arguments\n",
    "# Other arguments are not used in computation of derivatives\n",
    "\n",
    "def fn(x, a, b, c):\n",
    "    x1, x2 = x\n",
    "    return a + b*(x1 - 1)**2 + c*(x2 + 1)**2\n",
    "\n",
    "# Set derivative order\n",
    "\n",
    "n = 2\n",
    "\n",
    "# Set evaluation point\n",
    "\n",
    "x = torch.tensor([0.0, 0.0], dtype=dtype, device=device)\n",
    "\n",
    "# Set fixed parameters\n",
    "\n",
    "a, b, c = torch.tensor([1.0, 1.0, 1.0], dtype=dtype, device=device)\n",
    "\n",
    "# Compute only n'th derivative\n",
    "# Note, for given input & output the result is hessian\n",
    "\n",
    "value = nd.derivative(n, fn, x, a, b, c, intermediate=False, jacobian=functorch.jacfwd)\n",
    "print(value.cpu().numpy().tolist())\n",
    "\n",
    "# Compute all derivatives upto given order\n",
    "# Note, fuction value itself is referred as zeros order derivative\n",
    "# Output is a list of tensors (value, jacobian, hessian, ...)\n",
    "\n",
    "values = nd.derivative(n, fn, x, a, b, c, intermediate=True, jacobian=functorch.jacfwd)\n",
    "print(*[value.cpu().numpy().tolist() for value in values], sep=', ')\n",
    "\n",
    "# Compute jacobian and hessian with functorch\n",
    "\n",
    "print(fn(x, a, b, c).cpu().numpy().tolist(), \n",
    "      functorch.jacfwd(lambda x: fn(x, a, b, c))(x).cpu().numpy().tolist(), \n",
    "      functorch.hessian(lambda x: fn(x, a, b, c))(x).cpu().numpy().tolist(), \n",
    "      sep=', ')\n",
    "\n",
    "# Evaluate derivative table representation for a given deviation from the evaluation point\n",
    "\n",
    "dx = torch.tensor([+1.0, -1.0], dtype=dtype, device=device)\n",
    "print(nd.evaluate(values, [dx]).cpu().numpy())\n",
    "print(fn(x + dx, a, b, c).cpu().numpy())\n",
    "\n",
    "# Evaluate can be mapped over a set of deviation values\n",
    "\n",
    "print(functorch.vmap(lambda x: nd.evaluate(values, [x]))(torch.stack(5*[dx])).cpu().numpy().tolist())\n",
    "\n",
    "# Derivative can be mapped over a set of evaluation points\n",
    "# Note, the inputt function is expeted to return a tensor\n",
    "\n",
    "print(functorch.vmap(lambda x: nd.derivative(1, fn, x, a, b, c, intermediate=False))(torch.stack(5*[x])).cpu().numpy().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd190cb3-ed62-435b-8fea-4a5cb5ee4c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0], [[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]]\n",
      "\n",
      "[-1.0, -1.0, -1.0]\n",
      "[-1.0, -1.0, -1.0]\n"
     ]
    }
   ],
   "source": [
    "# Derivative\n",
    "\n",
    "# Input:  vector\n",
    "# Output: vector\n",
    "\n",
    "# Set test function\n",
    "# Note, the first function argument is a vector tensor\n",
    "# Input function can have other additional arguments\n",
    "# Other arguments (if any) are not used in computation of derivatives\n",
    "\n",
    "def fn(x):\n",
    "    x1, x2 = x\n",
    "    X1 = 1.0*x1 + 2.0*x2\n",
    "    X2 = 3.0*x1 + 4.0*x2\n",
    "    X3 = 5.0*x1 + 6.0*x2\n",
    "    return torch.stack([X1, X2, X3])\n",
    "\n",
    "# Set derivative order\n",
    "\n",
    "n = 1\n",
    "\n",
    "# Set evaluation point\n",
    "\n",
    "x = torch.tensor([0.0, 0.0], dtype=dtype, device=device)\n",
    "\n",
    "# Compute derivatives\n",
    "\n",
    "values = nd.derivative(n, fn, x)\n",
    "print(*[value.cpu().numpy().tolist() for value in values], sep=', ')\n",
    "print()\n",
    "\n",
    "# Evaluate derivative table representation for a given deviation from the evaluation point\n",
    "\n",
    "dx = torch.tensor([+1, -1], dtype=dtype, device=device)\n",
    "print(nd.evaluate(values, [dx]).cpu().numpy().tolist())\n",
    "print(fn(x + dx).cpu().numpy().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8701357-d23a-4daf-abe5-8333c4b2c2ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n",
      "[1, 2, 3, 1, 2, 3]\n",
      "[1, 2, 3, 1, 2, 3, 1, 2, 3]\n",
      "[1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3]\n",
      "[[[1.0, 1.0, 1.0], [1.0, 1.0, 1.0]]]\n",
      "[[[1.0, 1.0, 1.0], [1.0, 1.0, 1.0]]]\n",
      "[[[4.0, 4.0, 4.0], [4.0, 4.0, 4.0]]]\n",
      "[[[4.0, 4.0, 4.0], [4.0, 4.0, 4.0]]]\n",
      "[[[4.0, 4.0, 4.0], [4.0, 4.0, 4.0]]]\n"
     ]
    }
   ],
   "source": [
    "# Derivative\n",
    "\n",
    "# Input:  tensor\n",
    "# Output: tensor\n",
    "\n",
    "# Set test function\n",
    "\n",
    "def fn(x):\n",
    "    return 1 + x + x**2 + x**3\n",
    "\n",
    "# Set derivative order\n",
    "\n",
    "n = 3\n",
    "\n",
    "# Set evaluation point\n",
    "\n",
    "x = torch.zeros((1, 2, 3), dtype=dtype, device=device)\n",
    "\n",
    "# Compute derivatives\n",
    "# Note, output is a list of tensors\n",
    "\n",
    "values = nd.derivative(n, fn, x)\n",
    "print(*[list(value.shape) for value in values], sep='\\n')\n",
    "\n",
    "# Evaluate derivative table representation for a given deviation from the evaluation point\n",
    "# Note, evaluate function works with scalar or vector tensor input\n",
    "# One should compute derivatives of a wrapped function and reshape the result of evaluate\n",
    "\n",
    "# Set wrapped function\n",
    "\n",
    "def gn(x, shape):\n",
    "    return fn(x.reshape(shape)).flatten()\n",
    "\n",
    "print(fn(x).cpu().numpy().tolist())\n",
    "print(gn(x.flatten(), x.shape).reshape(x.shape).cpu().numpy().tolist())\n",
    "\n",
    "# Compute derivatives\n",
    "\n",
    "values = nd.derivative(n, gn, x.flatten(), x.shape)\n",
    "\n",
    "# Set deviation value\n",
    "\n",
    "dx = torch.ones_like(x)\n",
    "\n",
    "# Evaluate\n",
    "\n",
    "print(nd.evaluate(values, [dx.flatten()]).reshape(x.shape).cpu().numpy().tolist())\n",
    "print(gn((x + dx).flatten(), x.shape).reshape(x.shape).cpu().numpy().tolist())\n",
    "print(fn(x + dx).cpu().numpy().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fb30895-12e5-432b-a145-e4bca8f5932e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derivative\n",
    "\n",
    "# Input:  vector\n",
    "# Output: nested list of tensors\n",
    "\n",
    "# Set test function\n",
    "\n",
    "def fn(x):\n",
    "    x1, x2, x3, x4, x5, x6 = x\n",
    "    X1 = 1.0*x1 + 2.0*x2 + 3.0*x3\n",
    "    X2 = 4.0*x4 + 5.0*x5 + 6.0*x6\n",
    "    return [torch.stack([X1]), [torch.stack([X2])]]\n",
    "\n",
    "# Set derivative order\n",
    "\n",
    "n = 1\n",
    "\n",
    "# Set evaluation point\n",
    "\n",
    "x = torch.tensor([0.0, 0.0, 0.0, 0.0, 0.0, 0.0], dtype=dtype, device=device)\n",
    "\n",
    "# Compute derivatives\n",
    "\n",
    "values = nd.derivative(n, fn, x, intermediate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9bbcd01b-ce9f-49fc-99e1-7ab7b3191488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[1.0, 1.0], [1.0, 1.0]], [[1.0, 1.0], [1.0, 1.0]]]]\n",
      "[8.0]\n",
      "[8.0]\n",
      "(0, 0, 0, 0, 0, 0): [0.0]\n",
      "(0, 0, 0, 0, 1, 0): [0.0]\n",
      "(0, 0, 0, 0, 0, 1): [0.0]\n",
      "(0, 0, 1, 0, 0, 0): [0.0]\n",
      "(0, 0, 0, 1, 0, 0): [0.0]\n",
      "(0, 0, 1, 0, 1, 0): [0.0]\n",
      "(0, 0, 1, 0, 0, 1): [0.0]\n",
      "(0, 0, 0, 1, 1, 0): [0.0]\n",
      "(0, 0, 0, 1, 0, 1): [0.0]\n",
      "(1, 0, 0, 0, 0, 0): [0.0]\n",
      "(0, 1, 0, 0, 0, 0): [0.0]\n",
      "(1, 0, 0, 0, 1, 0): [0.0]\n",
      "(1, 0, 0, 0, 0, 1): [0.0]\n",
      "(0, 1, 0, 0, 1, 0): [0.0]\n",
      "(0, 1, 0, 0, 0, 1): [0.0]\n",
      "(1, 0, 1, 0, 0, 0): [0.0]\n",
      "(1, 0, 0, 1, 0, 0): [0.0]\n",
      "(0, 1, 1, 0, 0, 0): [0.0]\n",
      "(0, 1, 0, 1, 0, 0): [0.0]\n",
      "(1, 0, 1, 0, 1, 0): [1.0]\n",
      "(1, 0, 1, 0, 0, 1): [1.0]\n",
      "(1, 0, 0, 1, 1, 0): [1.0]\n",
      "(1, 0, 0, 1, 0, 1): [1.0]\n",
      "(0, 1, 1, 0, 1, 0): [1.0]\n",
      "(0, 1, 1, 0, 0, 1): [1.0]\n",
      "(0, 1, 0, 1, 1, 0): [1.0]\n",
      "(0, 1, 0, 1, 0, 1): [1.0]\n"
     ]
    }
   ],
   "source": [
    "# Derivative\n",
    "\n",
    "# Input:  vector, vector, vector\n",
    "# Output: vector\n",
    "\n",
    "# Set test function\n",
    "\n",
    "def fn(x, y, z):\n",
    "    x1, x2 = x\n",
    "    y1, y2 = y\n",
    "    z1, z2 = z\n",
    "    return torch.stack([(x1 + x2)*(y1 + y2)*(z1 + z2)])\n",
    "\n",
    "# Set derivative orders for x, y and z\n",
    "\n",
    "nx, ny, nz = 1, 1, 1\n",
    "\n",
    "# Set evaluation point\n",
    "# Note, evaluation point is a list of tensors\n",
    "\n",
    "x = torch.tensor([0.0, 0.0], dtype=dtype, device=device)\n",
    "y = torch.tensor([0.0, 0.0], dtype=dtype, device=device)\n",
    "z = torch.tensor([0.0, 0.0], dtype=dtype, device=device)\n",
    "\n",
    "# Compute n'th derivativ\n",
    "\n",
    "value = nd.derivative((nx, ny, nz), fn, x, y, z, intermediate=False)\n",
    "print(value.cpu().numpy().tolist())\n",
    "\n",
    "# Compute all derivatives upto given order\n",
    "\n",
    "values = nd.derivative((nx, ny, nz), fn, x, y, z, intermediate=True)\n",
    "\n",
    "# Evaluate derivative table representation for a given deviation from the evaluation point\n",
    "\n",
    "dx = torch.tensor([1.0, 1.0], dtype=dtype, device=device)\n",
    "dy = torch.tensor([1.0, 1.0], dtype=dtype, device=device)\n",
    "dz = torch.tensor([1.0, 1.0], dtype=dtype, device=device)\n",
    "print(nd.evaluate(values, [dx, dy, dz]).cpu().numpy().tolist())\n",
    "print(fn(x + dx, y + dy, z + dz).cpu().numpy().tolist())\n",
    "\n",
    "# Note, if the input function has vector arguments and returns a tensor, it can be repsented with series\n",
    "\n",
    "for key, value in nd.series(tuple(map(len, (x, y, z))), (nx, ny, nz), values).items():\n",
    "    print(f'{key}: {value.cpu().numpy().tolist()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b4d1b6e-7fa1-4879-a91a-9d6ab165fe73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[6.0, 4.0], [4.0, 10.0]]]\n",
      "[6.0]\n",
      "[4.0]\n",
      "[10.0]\n"
     ]
    }
   ],
   "source": [
    "# Redundancy free computation\n",
    "\n",
    "# Set test function\n",
    "\n",
    "def fn(x):\n",
    "    x1, x2 = x\n",
    "    return torch.stack([1.0*x1 + 2.0*x2 + 3.0*x1**2 + 4.0*x1*x2 + 5.0*x2**2])\n",
    "\n",
    "# Set derivative order\n",
    "\n",
    "n = 2\n",
    "\n",
    "# Set evaluation point\n",
    "\n",
    "x = torch.tensor([0.0, 0.0], dtype=dtype, device=device)\n",
    "\n",
    "# Compute n'th derivative\n",
    "\n",
    "value = nd.derivative(n, fn, x, intermediate=False)\n",
    "print(value.cpu().numpy().tolist())\n",
    "\n",
    "# Since derivatives are computed by nesting of jacobian function, redundant computations appear starting from the second order\n",
    "# Redundant computations can be avoided if all input arguments are scalar tensors\n",
    "\n",
    "def gn(x1, x2):\n",
    "    return fn(torch.stack([x1, x2]))\n",
    "\n",
    "print(nd.derivative((2, 0), gn, *x, intermediate=False).cpu().numpy().tolist())\n",
    "print(nd.derivative((1, 1), gn, *x, intermediate=False).cpu().numpy().tolist())\n",
    "print(nd.derivative((0, 2), gn, *x, intermediate=False).cpu().numpy().tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9d9fc1-9478-47be-9696-d8e30056d734",
   "metadata": {},
   "source": [
    "# Example-02: Derivative table representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9194ea1a-5a1b-402e-93b2-fdc55a9eca34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input function f: R^n x R^m x ... -> R^n is referred as a mapping\n",
    "# The first function argument is state, other arguments (used in computation of derivatives) and knobs\n",
    "# State and all knobs are vector-like tensors\n",
    "# Note, functions of this form can be used to model tranformations throught accelerator magnets\n",
    "\n",
    "# In this case, derivatives can be used to generate a (parametric) model of the input function\n",
    "# Function model can be represented as a derivative table or coefficients of monomials (series representation)\n",
    "\n",
    "# In this example, table representation is used to model transformation throught a sextupole accelerator magnet\n",
    "# Table is computed with respect to state variables (phase space variables) and knobs (magnet strength and length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87b75205-6af6-4775-bcd4-4b856b1c47d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Import\n",
    "\n",
    "import numpy\n",
    "import torch\n",
    "import functorch\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import ndtorch.ndtorch as nd\n",
    "\n",
    "torch.set_printoptions(precision=12, sci_mode=True)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b23e7f22-280b-4f1b-8821-317be9523f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set data type and device\n",
    "\n",
    "dtype = torch.float64\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6aec6e68-a848-4713-8579-5a2aa56ec962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping (sextupole accelerator magnet transformation)\n",
    "# Given initial state, magnet strength and length, state is propagated using explicit symplectic integration\n",
    "# Number of integration steps is set by count parameter, integration step length is length/count\n",
    "\n",
    "def mapping(x, k, l, count=100):\n",
    "    (qx, px, qy, py), (k, ), (l, ) = x, k, l/(2.0*count)\n",
    "    for _ in range(count):\n",
    "        qx, qy = qx + l*px, qy + l*py\n",
    "        px, py = px - 2.0*l*k*(qx**2 - qy**2), py + 2.0*l*k*qx*qy\n",
    "        qx, qy = qx + l*px, qy + l*py\n",
    "    return torch.stack([qx, px, qy, py])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b784d45e-2d4f-4e32-a409-7a9c53c5256f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4])\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4, 4])\n",
      "torch.Size([4, 4, 4, 4])\n",
      "torch.Size([4, 4, 4, 4, 4])\n",
      "torch.Size([4, 4, 4, 4, 4, 4])\n",
      "torch.Size([4, 4, 4, 4, 4, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "# Table representation (state)\n",
    "\n",
    "# Set evaluation point & parameters\n",
    "\n",
    "x = torch.tensor([0.0, 0.0, 0.0, 0.0], dtype=dtype, device=device)\n",
    "k = torch.tensor([10.0], dtype=dtype, device=device)\n",
    "l = torch.tensor([0.1], dtype=dtype, device=device)\n",
    "\n",
    "# Compute derivatives (table representation)\n",
    "# Since derivatives are computed only with respect to the state, output table is a list of tensors\n",
    "\n",
    "t = nd.derivative(6, mapping, x, k, l)\n",
    "\n",
    "print(*[element.shape for element in t], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4107f58-8cfe-45e4-9061-0d21839db12d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00010000041666220641, 0.001000006666736112, 0.00010000016667544442, 5.000018331681037e-09]\n",
      "[0.00010000041666220632, 0.001000006666736112, 0.00010000016667544451, 5.000018331681034e-09]\n"
     ]
    }
   ],
   "source": [
    "# Compare table and exact mapping near the evaluation point (change order to observe convergence)\n",
    "# Note, table transformation is not symplectic\n",
    "\n",
    "dx = torch.tensor([0.0, 0.001, 0.0001, 0.0], dtype=dtype, device=device)\n",
    "\n",
    "print(nd.evaluate(t, [dx]).cpu().tolist())\n",
    "print(mapping(x + dx, k, l).cpu().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fbc3b8a-ddb4-47a9-90a0-3b4e4799636d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,), (1,), (2,), (3,), (4,), (5,), (6,)]\n"
     ]
    }
   ],
   "source": [
    "# Each bottom element (tensor) in the (flattend) derivative table is assosiated with a signature\n",
    "# Signature is a tuple of derivative orders\n",
    "\n",
    "print(nd.signature(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee668111-a0aa-4ba9-870d-e2409880ad47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.  0.1 0.  0. ]\n",
      " [0.  1.  0.  0. ]\n",
      " [0.  0.  1.  0.1]\n",
      " [0.  0.  0.  1. ]]\n"
     ]
    }
   ],
   "source": [
    "# For a given signature, corresponding element can be extracted or changed with get/set functions\n",
    "\n",
    "print(nd.get(t, (1, )).cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c688731-6776-4824-a275-b9ed00d111d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 0 0 0]\n",
      " [1 1 0 0]\n",
      " [1 0 1 0]\n",
      " [1 0 0 1]\n",
      " [1 1 0 0]\n",
      " [0 2 0 0]\n",
      " [0 1 1 0]\n",
      " [0 1 0 1]\n",
      " [1 0 1 0]\n",
      " [0 1 1 0]\n",
      " [0 0 2 0]\n",
      " [0 0 1 1]\n",
      " [1 0 0 1]\n",
      " [0 1 0 1]\n",
      " [0 0 1 1]\n",
      " [0 0 0 2]]\n"
     ]
    }
   ],
   "source": [
    "# Each bottom element is related to monomials\n",
    "# For given order, monomial indices with repetitions can be computed\n",
    "# These repetitions account for evaluation of the same partial derivatives with diffenent orders, e.g. df/dxdy vs df/dydx\n",
    "\n",
    "print(nd.index(4, 2).cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1f332f5-f3a6-4d68-91a1-611b09d08385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.00000417e-04 1.00000667e-03 1.00000167e-04 5.00001833e-09]\n",
      "[1.00000417e-04 1.00000667e-03 1.00000167e-04 5.00001833e-09]\n",
      "[1.00000417e-04 1.00000667e-03 1.00000167e-04 5.00001833e-09]\n"
     ]
    }
   ],
   "source": [
    "# Explicit evaluation\n",
    "\n",
    "print(nd.evaluate(t, [dx]).cpu().numpy())\n",
    "print((t[0] + t[1] @ dx + 1/2 * t[2] @ dx @ dx + 1/2 * 1/3 * t[3] @ dx @ dx @ dx + 1/2 * 1/3 * 1/4 * t[4] @ dx @ dx @ dx @ dx + 1/2 * 1/3 * 1/4 * 1/5 * t[5] @ dx @ dx @ dx @ dx @ dx + 1/2 * 1/3 * 1/4 * 1/5 * 1/6 * t[6] @ dx @ dx @ dx @ dx @ dx @ dx).cpu().numpy())\n",
    "print((t[0] + (t[1] + 1/2 * (t[2] + 1/3 * (t[3] + 1/4 * (t[4] + 1/5 * (t[5] + 1/6 * t[6] @ dx) @ dx) @ dx) @ dx) @ dx) @ dx).cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94bfcd0d-1ca0-4f05-98d3-7b889a5cbc69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.  0.  0.  0. ]\n",
      " [0.1 1.  0.  0. ]\n",
      " [0.  0.  1.  0. ]\n",
      " [0.  0.  0.1 1. ]]\n"
     ]
    }
   ],
   "source": [
    "# Series representation can be generated from a given table\n",
    "# This representation stores monomial powers and corresponding coefficients\n",
    "\n",
    "s = nd.series((4, ), (6, ), t)\n",
    "print(torch.stack([s[(1, 0, 0, 0)], s[(0, 1, 0, 0)], s[(0, 0, 1, 0)], s[(0, 0, 0, 1)]]).cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9325c49-1a2d-4d37-bef0-f59b5d2cfe19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.00000417e-04 1.00000667e-03 1.00000167e-04 5.00001833e-09]\n",
      "[1.00000417e-04 1.00000667e-03 1.00000167e-04 5.00001833e-09]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate series\n",
    "\n",
    "print(nd.evaluate(t, [dx]).cpu().numpy())\n",
    "print(nd.evaluate(s, [dx]).cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b07492b-5f44-4925-a18f-310ffdbbd8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table representation (state & knobs)\n",
    "\n",
    "# Set evaluation point\n",
    "\n",
    "x = torch.tensor([0.0, 0.0, 0.0, 0.0], dtype=dtype, device=device)\n",
    "k = torch.tensor([10.0], dtype=dtype, device=device)\n",
    "l = torch.tensor([0.1], dtype=dtype, device=device)\n",
    "\n",
    "# Compute derivatives (table representation)\n",
    "# Since derivatives are computed with respect to state and knobs, output table is a nested list of tensors\n",
    "\n",
    "t = nd.derivative((6, 1, 1), mapping, x, k, l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f40ca1b-ec32-4988-8cf2-bb282ce2081d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.  0.1 0.  0. ]\n",
      " [0.  1.  0.  0. ]\n",
      " [0.  0.  1.  0.1]\n",
      " [0.  0.  0.  1. ]]\n"
     ]
    }
   ],
   "source": [
    "# In this case, bottom table element signature is a tuple with several integers\n",
    "\n",
    "print(nd.get(t, (1, 0, 0)).cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c1324ac-7134-487d-8d01-bdc27f5c2bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00010000041666220641, 0.001000006666736112, 0.00010000016667544442, 5.000018331681037e-09]\n",
      "[0.00010100042756197639, 0.0010000067334053492, 0.0001000001733924745, 5.151019293264671e-09]\n",
      "[0.00010100042756163604, 0.0010000067323920011, 0.0001000001734431457, 5.151524301990432e-09]\n"
     ]
    }
   ],
   "source": [
    "# Compare table and exact mapping near evaluation point (change order to observe convergence)\n",
    "# Note, table transofrmation is not symplectic\n",
    "\n",
    "dx = torch.tensor([0.0, 0.001, 0.0001, 0.0], dtype=dtype, device=device)\n",
    "dk = torch.tensor([0.1], dtype=dtype, device=device)\n",
    "dl = torch.tensor([0.001], dtype=dtype, device=device)\n",
    "\n",
    "print(nd.evaluate(t, [dx, 0.0*dk, 0.0*dl]).cpu().tolist())\n",
    "print(nd.evaluate(t, [dx, 1.0*dk, 1.0*dl]).cpu().tolist())\n",
    "print(mapping(x + dx, k + dk, l + dl).cpu().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e031c18b-8836-45ee-acc5-0d05d56d58ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0, 0)\n",
      "(0, 0, 1)\n",
      "(0, 1, 0)\n",
      "(0, 1, 1)\n",
      "(1, 0, 0)\n",
      "(1, 0, 1)\n",
      "(1, 1, 0)\n",
      "(1, 1, 1)\n",
      "(2, 0, 0)\n",
      "(2, 0, 1)\n",
      "(2, 1, 0)\n",
      "(2, 1, 1)\n",
      "(3, 0, 0)\n",
      "(3, 0, 1)\n",
      "(3, 1, 0)\n",
      "(3, 1, 1)\n",
      "(4, 0, 0)\n",
      "(4, 0, 1)\n",
      "(4, 1, 0)\n",
      "(4, 1, 1)\n",
      "(5, 0, 0)\n",
      "(5, 0, 1)\n",
      "(5, 1, 0)\n",
      "(5, 1, 1)\n",
      "(6, 0, 0)\n",
      "(6, 0, 1)\n",
      "(6, 1, 0)\n",
      "(6, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "# Each bottom element (tensor) in the (flattend) derivative table is assosiated with a signature\n",
    "# Signature is a tuple of derivative orders\n",
    "\n",
    "print(*[index for index in nd.signature(t)], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9dde6d2-0942-4a95-94c1-f7422d64e33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.42877133e-06 9.99755017e-05 0.00000000e+00 0.00000000e+00]\n",
      "\n",
      "[1.01000428e-04 1.00000673e-03 1.00000173e-04 5.15101929e-09]\n",
      "[1.01000428e-04 1.00000673e-03 1.00000173e-04 5.15101929e-09]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute series\n",
    "\n",
    "s = nd.series((4, 1, 1), (6, 1, 1), t)\n",
    "\n",
    "# Keys are generalized monomials\n",
    "\n",
    "print(s[(1, 1, 1, 1, 1, 1)].cpu().numpy())\n",
    "print()\n",
    "\n",
    "# Evaluate series\n",
    "\n",
    "print(nd.evaluate(t, [dx, dk, dl]).cpu().numpy())\n",
    "print(nd.evaluate(s, [dx, dk, dl]).cpu().numpy())\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046b2a78-7ca1-4622-938e-4a2cd51fa85f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "myt0_gMIOq7b",
    "5d97819c"
   ],
   "name": "03_frequency.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
